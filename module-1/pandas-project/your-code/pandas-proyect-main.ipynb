{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Shark Attack Incidents\n",
    "\n",
    "## First pandas proyect\n",
    "\n",
    " ### Step 1.- Data acquisition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At importing dataset it is necessary to change encoding to open it.\n",
    "# It is not encoded as utf-8, so it is necessary to open it with the correct encoding.\n",
    "df = pd.read_csv('input/GSAF5.csv',encoding= 'iso-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.- Data cleaning:\n",
    "\n",
    "The first thing that will be done is to do an overvierw of what kind of data do we have and the quality of It."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>Area</th>\n",
       "      <th>Location</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Injury</th>\n",
       "      <th>Fatal (Y/N)</th>\n",
       "      <th>Time</th>\n",
       "      <th>Species</th>\n",
       "      <th>Investigator or Source</th>\n",
       "      <th>pdf</th>\n",
       "      <th>href formula</th>\n",
       "      <th>href</th>\n",
       "      <th>Case Number.1</th>\n",
       "      <th>Case Number.2</th>\n",
       "      <th>original order</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "      <th>Unnamed: 23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016.09.18.c</td>\n",
       "      <td>18-Sep-16</td>\n",
       "      <td>2016</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Florida</td>\n",
       "      <td>New Smyrna Beach, Volusia County</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>male</td>\n",
       "      <td>M</td>\n",
       "      <td>16</td>\n",
       "      <td>Minor injury to thigh</td>\n",
       "      <td>N</td>\n",
       "      <td>13h00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Orlando Sentinel, 9/19/2016</td>\n",
       "      <td>2016.09.18.c-NSB.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2016.09.18.c</td>\n",
       "      <td>2016.09.18.c</td>\n",
       "      <td>5993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016.09.18.b</td>\n",
       "      <td>18-Sep-16</td>\n",
       "      <td>2016</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Florida</td>\n",
       "      <td>New Smyrna Beach, Volusia County</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>Chucky Luciano</td>\n",
       "      <td>M</td>\n",
       "      <td>36</td>\n",
       "      <td>Lacerations to hands</td>\n",
       "      <td>N</td>\n",
       "      <td>11h00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Orlando Sentinel, 9/19/2016</td>\n",
       "      <td>2016.09.18.b-Luciano.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2016.09.18.b</td>\n",
       "      <td>2016.09.18.b</td>\n",
       "      <td>5992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016.09.18.a</td>\n",
       "      <td>18-Sep-16</td>\n",
       "      <td>2016</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Florida</td>\n",
       "      <td>New Smyrna Beach, Volusia County</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>male</td>\n",
       "      <td>M</td>\n",
       "      <td>43</td>\n",
       "      <td>Lacerations to lower leg</td>\n",
       "      <td>N</td>\n",
       "      <td>10h43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Orlando Sentinel, 9/19/2016</td>\n",
       "      <td>2016.09.18.a-NSB.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2016.09.18.a</td>\n",
       "      <td>2016.09.18.a</td>\n",
       "      <td>5991</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016.09.17</td>\n",
       "      <td>17-Sep-16</td>\n",
       "      <td>2016</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>Thirteenth Beach</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>Rory Angiolella</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Struck by fin on chest &amp; leg</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Age, 9/18/2016</td>\n",
       "      <td>2016.09.17-Angiolella.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2016.09.17</td>\n",
       "      <td>2016.09.17</td>\n",
       "      <td>5990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016.09.15</td>\n",
       "      <td>16-Sep-16</td>\n",
       "      <td>2016</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>Bells Beach</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>male</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No injury: Knocked off board by shark</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2 m shark</td>\n",
       "      <td>The Age, 9/16/2016</td>\n",
       "      <td>2016.09.16-BellsBeach.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2016.09.16</td>\n",
       "      <td>2016.09.15</td>\n",
       "      <td>5989</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Case Number       Date  Year        Type    Country      Area  \\\n",
       "0  2016.09.18.c  18-Sep-16  2016  Unprovoked        USA   Florida   \n",
       "1  2016.09.18.b  18-Sep-16  2016  Unprovoked        USA   Florida   \n",
       "2  2016.09.18.a  18-Sep-16  2016  Unprovoked        USA   Florida   \n",
       "3    2016.09.17  17-Sep-16  2016  Unprovoked  AUSTRALIA  Victoria   \n",
       "4    2016.09.15  16-Sep-16  2016  Unprovoked  AUSTRALIA  Victoria   \n",
       "\n",
       "                           Location Activity             Name Sex   Age  \\\n",
       "0  New Smyrna Beach, Volusia County  Surfing             male    M   16   \n",
       "1  New Smyrna Beach, Volusia County  Surfing   Chucky Luciano    M   36   \n",
       "2  New Smyrna Beach, Volusia County  Surfing             male    M   43   \n",
       "3                  Thirteenth Beach  Surfing  Rory Angiolella    M  NaN   \n",
       "4                       Bells Beach  Surfing             male    M  NaN   \n",
       "\n",
       "                                  Injury Fatal (Y/N)   Time   Species   \\\n",
       "0                  Minor injury to thigh           N  13h00        NaN   \n",
       "1                   Lacerations to hands           N  11h00        NaN   \n",
       "2               Lacerations to lower leg           N  10h43        NaN   \n",
       "3           Struck by fin on chest & leg           N    NaN        NaN   \n",
       "4  No injury: Knocked off board by shark           N    NaN  2 m shark   \n",
       "\n",
       "        Investigator or Source                        pdf  \\\n",
       "0  Orlando Sentinel, 9/19/2016       2016.09.18.c-NSB.pdf   \n",
       "1  Orlando Sentinel, 9/19/2016   2016.09.18.b-Luciano.pdf   \n",
       "2  Orlando Sentinel, 9/19/2016       2016.09.18.a-NSB.pdf   \n",
       "3           The Age, 9/18/2016  2016.09.17-Angiolella.pdf   \n",
       "4           The Age, 9/16/2016  2016.09.16-BellsBeach.pdf   \n",
       "\n",
       "                                        href formula  \\\n",
       "0  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "1  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "2  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "3  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "4  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "\n",
       "                                                href Case Number.1  \\\n",
       "0  http://sharkattackfile.net/spreadsheets/pdf_di...  2016.09.18.c   \n",
       "1  http://sharkattackfile.net/spreadsheets/pdf_di...  2016.09.18.b   \n",
       "2  http://sharkattackfile.net/spreadsheets/pdf_di...  2016.09.18.a   \n",
       "3  http://sharkattackfile.net/spreadsheets/pdf_di...    2016.09.17   \n",
       "4  http://sharkattackfile.net/spreadsheets/pdf_di...    2016.09.16   \n",
       "\n",
       "  Case Number.2  original order Unnamed: 22 Unnamed: 23  \n",
       "0  2016.09.18.c            5993         NaN         NaN  \n",
       "1  2016.09.18.b            5992         NaN         NaN  \n",
       "2  2016.09.18.a            5991         NaN         NaN  \n",
       "3    2016.09.17            5990         NaN         NaN  \n",
       "4    2016.09.15            5989         NaN         NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Let's set the options to be able to see all clomuns while printing our dataset. \n",
    "'''\n",
    "pd.set_option('display.max_columns', 24)\n",
    "'''\n",
    "Afterwrds lets check our data and make a decission about what data do we have,\n",
    "what is the quality of them and choose the column that can be usefull to get interesting\n",
    "conclusions about this dataset.\n",
    "'''\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we have a lot of data: \n",
    "\n",
    "About kinds of sharks, when it attacked people, where it was, what was this people doing while the attack was performed, the injuries and if it was fatal or not, name of the person, a refference about the source of the data aqduisition and other data about sources and files refferences.\n",
    "\n",
    "Let's check the quality of this data to be able to know what can be useful for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of our dataframe is: (5992, 24)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Country                     43\n",
       "Area                       402\n",
       "Location                   496\n",
       "Activity                   527\n",
       "Name                       200\n",
       "Sex                        567\n",
       "Age                       2681\n",
       "Injury                      27\n",
       "Fatal (Y/N)                 19\n",
       "Time                      3213\n",
       "Species                   2934\n",
       "Investigator or Source      15\n",
       "href formula                 1\n",
       "href                         3\n",
       "Unnamed: 22               5991\n",
       "Unnamed: 23               5990\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets check about null values in columns and compare them with the size of our dataframe:\n",
    "print('The size of our dataframe is:',df.shape)\n",
    "null_values = df.isnull().sum()\n",
    "null_values[null_values > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are two columns with almost 95% of rows with NaN values, this is unusefull for us.\n",
    "\n",
    "Afterwars we have tree columns with near to half of the values NaN, but it could contain data that can be used to get any conclussion as type of shark, the moment of the day when the attack was performed or the shark species which attacked.\n",
    "Anyway, as the dataset has other data stored with much more information in which we can focus on. We will discard Age, Time and Species.\n",
    "\n",
    "Other columns as Name, href formula, pdf or href have data that can't be used to make decissions or get conclussions. They have so heterogeneus strings data or in case of Case Number, case Number.1 and case Number.2 the data are so similar to the other, so we will discard them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Case Number', 'Date', 'Year', 'Type', 'Country', 'Area', 'Location',\n",
      "       'Activity', 'Sex ', 'Injury', 'Fatal (Y/N)', 'Investigator or Source'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Country                    43\n",
       "Area                      402\n",
       "Location                  496\n",
       "Activity                  527\n",
       "Sex                       567\n",
       "Injury                     27\n",
       "Fatal (Y/N)                19\n",
       "Investigator or Source     15\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We drop columns with more that 2600 Nan elements, that includes 'Unnamed: 22','Unnamed: 23',\\\n",
    "'Age', 'Time'and 'Species'\n",
    "'''\n",
    "drop_cols = list(null_values[null_values >=2600].index)\n",
    "df = df.drop(drop_cols, axis=1)\n",
    "'''\n",
    "Next thing to do is drop other columns, that even having great quantity of data they are heterogeneous\n",
    "strings or simiar to other columns.\n",
    "'''\n",
    "drop_cols = ['Name','href','href formula','pdf','Case Number.1','Case Number.2','original order']\n",
    "df = df.drop(columns=drop_cols, axis=1)\n",
    "#Let's check atual columns\n",
    "print(df.columns)\n",
    "#Let's check about NaN reaminig values.\n",
    "null_values = df.isnull().sum()\n",
    "null_values[null_values > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have less NaN values and data that could be useful for us. Now it is necessary to check the column names in order to make easy work with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Case Number', 'Date', 'Year', 'Type', 'Country', 'Area', 'Location',\n",
       "       'Activity', 'Sex', 'Injury', 'Fatal', 'Source'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Renaming wrong named columns\n",
    "df.rename(columns={'Sex ':'Sex','Fatal (Y/N)':'Fatal','Investigator or Source':'Source'},inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this point it is necessary to start cleaning the received data. Let's start for the most homogeneus columns in order to do first the easier tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unprovoked      4386\n",
       "Provoked         557\n",
       "Invalid          519\n",
       "Sea Disaster     220\n",
       "Boat             200\n",
       "Boating          110\n",
       "Name: Type, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Type columns has values with the same meaning called by differents ways.\n",
    "df['Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unprovoked      4386\n",
       "Provoked         557\n",
       "Invalid          519\n",
       "Boat             310\n",
       "Sea Disaster     220\n",
       "Name: Type, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Type'] = df['Type'].str.replace('Boating', 'Boat')\n",
    "df['Type'].value_counts()\n",
    "#Now Type column has uniform values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N          4315\n",
       "Y          1552\n",
       "UNKNOWN      94\n",
       " N            8\n",
       "N             1\n",
       "F             1\n",
       "#VALUE!       1\n",
       "n             1\n",
       "Name: Fatal, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets check Fatal column in order to clean those Data:\n",
    "df['Fatal'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N          4325\n",
       "Y          1553\n",
       "UNKNOWN      95\n",
       "Name: Fatal, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Thre only should have three values: Y, N and UNKNOWN so let's clean them:\n",
    "# first it is necessary to suprime empty spaces: \n",
    "df['Fatal'] = df['Fatal'].str.replace(' ', '').str.replace('F','Y').str.replace('#VALUE!','UNKNOWN').str.replace('n','N')\n",
    "df['Fatal'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M      4835\n",
       "F       585\n",
       "M         2\n",
       ".         1\n",
       "lli       1\n",
       "N         1\n",
       "Name: Sex, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let'se check the values in sex column:\n",
    "df['Sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M    4837\n",
       "F     585\n",
       "Name: Sex, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sex'] = df['Sex'].str.replace(' ', '')\n",
    "\n",
    "df = df.drop(df[(df['Sex'] == 'lli') | (df['Sex'] == '.') | (df['Sex'] == 'N')].index)\n",
    "df['Sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5989, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5882, 12)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#subdf = df[df.isnull().sum().index]\n",
    "#subdf =[i for i in df['Location'] if i.isnull()]\n",
    "from modules.functions import get_poor_rows\n",
    "from modules.functions import drop_by_index\n",
    "bad_indexes = get_poor_rows(df) \n",
    "\n",
    "df = df.drop(df.iloc[bad_indexes].index)\n",
    "df.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5992, 22)\n",
      "23109\n",
      "(4137, 22)\n",
      "9102\n"
     ]
    }
   ],
   "source": [
    "# Let's check the quality of data according to date ranges. It is possible that the quality of old data were not so good for the analysis that we are trying to do.\n",
    "#df['Year'].value_counts()\n",
    "filtered_dataframe = df[df[\"Year\"] < 2000]\n",
    "print(df.shape)\n",
    "print(null_values[null_values>0].values.sum())\n",
    "print(filtered_dataframe.shape)\n",
    "null_values2 = filtered_dataframe.isnull().sum()\n",
    "print(null_values2[null_values2>0].values.sum())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
